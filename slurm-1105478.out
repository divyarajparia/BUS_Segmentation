==========================================
SLURM_JOB_ID = 1105478
SLURM_JOB_NODELIST = d14-18
TMPDIR = /tmp/SLURM_1105478
==========================================
ğŸ¥ FINAL Joint Image+Mask Diffusion Training
======================================================================
ğŸ“± Device: cuda
ğŸ“Š Loading from CSV: 485 samples
âœ… Successfully loaded 485 valid samples
   Benign: 330 samples (68.0%)
   Malignant: 155 samples (32.0%)

ğŸ¯ Training Configuration:
   Dataset size: 485 samples
   Batch size: 16
   Epochs: 50
   Learning rate: 0.0001
   Model parameters: 32,047,618
   Training batches per epoch: 31

ğŸš€ Starting Training...
Epoch 1/50:   0%|          | 0/31 [00:00<?, ?it/s]Epoch 1/50:   0%|          | 0/31 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "FINAL_train_joint_diffusion_server.py", line 417, in <module>
    train_joint_diffusion() 
  File "FINAL_train_joint_diffusion_server.py", line 360, in train_joint_diffusion
    loss = diffusion.p_losses(model, x_start, t, class_labels)
  File "FINAL_train_joint_diffusion_server.py", line 250, in p_losses
    x_noisy = self.q_sample(x_start, t, noise)
  File "FINAL_train_joint_diffusion_server.py", line 245, in q_sample
    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
