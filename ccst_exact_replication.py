#!/usr/bin/env python3
"""
CCST (Cross-Client Style Transfer) - PROPER AdaIN Implementation
Based on the paper: "Exploring Cross-Client Style Transfer for Medical Image Segmentation"

This implementation follows the original CCST methodology with PROPER AdaIN style transfer
including trained decoder for high-quality medical image style transfer.
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, models
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import pandas as pd
from PIL import Image
import numpy as np
from tqdm import tqdm
import argparse
import random
from collections import defaultdict
from torchvision.transforms.functional import to_pil_image

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ============================================================================
# PROPER AdaIN Decoder (Following Original AdaIN Paper)
# ============================================================================

class AdaINDecoder(nn.Module):
    """
    Proper AdaIN Decoder Network
    Mirror of VGG19 encoder up to relu4_1 with learned parameters
    """
    def __init__(self, in_channels=512):
        super(AdaINDecoder, self).__init__()
        
        # Decoder layers (mirror of VGG19 encoder)
        self.decoder = nn.Sequential(
            # From relu4_1 (512 channels) back to image
            nn.ReflectionPad2d(1),
            nn.Conv2d(in_channels, 256, 3, 1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='nearest'),
            
            nn.ReflectionPad2d(1),
            nn.Conv2d(256, 256, 3, 1),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(256, 256, 3, 1),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(256, 256, 3, 1),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(256, 128, 3, 1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='nearest'),
            
            nn.ReflectionPad2d(1),
            nn.Conv2d(128, 128, 3, 1),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(128, 64, 3, 1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='nearest'),
            
            nn.ReflectionPad2d(1),
            nn.Conv2d(64, 64, 3, 1),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(64, 3, 3, 1),  # RGB output
        )
        
    def forward(self, x):
        return self.decoder(x)

class ProperAdaINStyleTransfer(nn.Module):
    """
    PROPER AdaIN Style Transfer Implementation
    - VGG19 encoder (pre-trained, frozen)
    - AdaIN transformation
    - Trained decoder for high-quality reconstruction
    """
    def __init__(self, device='cuda'):
        super(ProperAdaINStyleTransfer, self).__init__()
        self.device = device
        
        # VGG19 encoder (pre-trained)
        vgg19 = models.vgg19(pretrained=True).features
        self.encoder = nn.Sequential(*list(vgg19.children())[:21]).to(device)
        
        # Freeze encoder
        for param in self.encoder.parameters():
            param.requires_grad = False
        self.encoder.eval()
        
        # AdaIN decoder
        self.decoder = AdaINDecoder().to(device)
        
        # Load pre-trained decoder weights if available, otherwise train
        self._initialize_decoder()
        
    def _initialize_decoder(self):
        """Initialize decoder with proper weights"""
        # Initialize with Xavier/He initialization
        for m in self.decoder.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def calc_mean_std(self, feat, eps=1e-5):
        """Calculate mean and std for style statistics"""
        size = feat.size()
        assert len(size) == 4
        N, C = size[:2]
        feat_var = feat.view(N, C, -1).var(dim=2) + eps
        feat_std = feat_var.sqrt().view(N, C, 1, 1)
        feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)
        return feat_mean, feat_std
    
    def adain(self, content_feat, style_mean, style_std):
        """
        Adaptive Instance Normalization
        AdaIN(Fc, Fs) = Ïƒ(Fs) * (Fc - Î¼(Fc)) / Ïƒ(Fc) + Î¼(Fs)
        """
        content_mean, content_std = self.calc_mean_std(content_feat)
        
        # Normalize content features
        normalized_feat = (content_feat - content_mean) / content_std
        
        # Apply style statistics
        stylized_feat = normalized_feat * style_std + style_mean
        
        return stylized_feat
    
    def train_decoder_on_reconstruction(self, dataloader, num_epochs=5):
        """
        Train decoder for reconstruction quality
        This ensures high-quality image generation
        """
        print("ðŸŽ“ Training AdaIN decoder for high-quality reconstruction...")
        
        self.decoder.train()
        optimizer = optim.Adam(self.decoder.parameters(), lr=1e-4)
        mse_loss = nn.MSELoss()
        
        for epoch in range(num_epochs):
            total_loss = 0
            for batch_idx, images in enumerate(tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")):
                if isinstance(images, (list, tuple)):
                    images = images[0]  # Handle dataset returning tuples
                
                images = images.to(self.device)
                
                # Forward pass: encode then decode
                with torch.no_grad():
                    features = self.encoder(images)
                
                # Decode features back to images
                reconstructed = self.decoder(features)
                
                # Ensure same size for loss calculation
                if reconstructed.size() != images.size():
                    reconstructed = F.interpolate(reconstructed, size=images.shape[-2:], mode='bilinear', align_corners=False)
                
                # Reconstruction loss
                loss = mse_loss(reconstructed, images)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                # Break after reasonable number of batches for efficiency
                if batch_idx >= 100:  # Train on subset for efficiency
                    break
            
            avg_loss = total_loss / min(len(dataloader), 100)
            print(f"   Epoch {epoch+1}: Reconstruction Loss = {avg_loss:.4f}")
        
        self.decoder.eval()
        print("âœ… Decoder training completed!")
    
    def forward(self, content_image, style_mean, style_std, alpha=1.0):
        """
        Forward pass for style transfer
        """
        # Extract content features
        content_feat = self.encoder(content_image)
        
        # Apply AdaIN
        stylized_feat = self.adain(content_feat, style_mean, style_std)
        
        # Interpolate between original and stylized features
        if alpha < 1.0:
            stylized_feat = alpha * stylized_feat + (1 - alpha) * content_feat
        
        # Decode to image
        stylized_image = self.decoder(stylized_feat)
        
        # Convert to grayscale for medical images
        if stylized_image.size(1) == 3:  # RGB
            # Convert RGB to grayscale using standard weights
            stylized_image = 0.299 * stylized_image[:, 0:1] + 0.587 * stylized_image[:, 1:2] + 0.114 * stylized_image[:, 2:3]
        
        return stylized_image

# ============================================================================
# CCST Style Extractor with Proper AdaIN
# ============================================================================

class CCSTStyleExtractor:
    """
    CCST Style Extractor using PROPER AdaIN methodology
    """
    
    def __init__(self, device='cuda'):
        self.device = device
        self.style_transfer_model = ProperAdaINStyleTransfer(device)
        print(f"âœ… PROPER CCST Style Extractor initialized on {device}")
    
    def extract_overall_domain_style(self, dataset_path, csv_file, J_samples=None):
        """
        Extract overall domain style from a dataset using VGG19 features.
        """
        print(f"ðŸŽ¨ Extracting overall domain style from {dataset_path}")
        
        # Load CSV file
        csv_path = os.path.join(dataset_path, csv_file)
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        df = pd.read_csv(csv_path)
        print(f"   ðŸ“Š Found {len(df)} samples in CSV")
        
        # Limit samples if specified
        if J_samples and J_samples < len(df):
            df = df.sample(n=J_samples, random_state=42)
            print(f"   ðŸ”€ Randomly selected {J_samples} samples for style extraction")
        
        # Image transforms for VGG19
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel for VGG
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # Extract features from all images
        all_features = []
        valid_count = 0
        
        for idx, row in df.iterrows():
            try:
                # Load image - handle different dataset formats
                image_filename = row['image_path']
                
                if ' ' in image_filename:
                    if '(' in image_filename:
                        # BUSI format: "benign (1).png"
                        class_type = image_filename.split()[0]
                        img_path = os.path.join(dataset_path, class_type, 'image', image_filename)
                    else:
                        # BUS-UCLM format: "benign image.png"
                        class_type = image_filename.split()[0]
                        image_name = image_filename.split()[1]
                        img_path = os.path.join(dataset_path, class_type, 'images', image_name)
                else:
                    # Fallback format
                    img_path = os.path.join(dataset_path, image_filename)
                
                if not os.path.exists(img_path):
                    print(f"   âš ï¸ Image not found: {img_path}")
                    continue
                
                image = Image.open(img_path).convert('L')  # Convert to grayscale
                image_tensor = transform(image).unsqueeze(0).to(self.device)
                
                # Extract VGG features
                with torch.no_grad():
                    features = self.style_transfer_model.encoder(image_tensor)
                    all_features.append(features)
                    valid_count += 1
                    
            except Exception as e:
                print(f"   âš ï¸ Error processing {row['image_path']}: {str(e)}")
                continue
        
        if not all_features:
            raise ValueError("No valid images found for style extraction")
        
        # Concatenate all features
        all_features = torch.cat(all_features, dim=0)
        
        # Calculate domain-level statistics
        domain_mean, domain_std = self.style_transfer_model.calc_mean_std(all_features)
        
        # Average across all samples to get domain-level statistics
        domain_style = {
            'mean': domain_mean.mean(dim=0, keepdim=True),
            'std': domain_std.mean(dim=0, keepdim=True)
        }
        
        print(f"   âœ… Extracted domain style from {len(all_features)} valid images")
        print(f"   ðŸ“ Style shape: mean={domain_style['mean'].shape}, std={domain_style['std'].shape}")
        
        return domain_style
    
    def train_decoder_with_data(self, dataset_path, csv_file, batch_size=8):
        """
        Train the decoder using actual medical images for high-quality reconstruction
        """
        # Create dataset for decoder training
        train_dataset = DecoderTrainingDataset(dataset_path, csv_file)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # Train decoder
        self.style_transfer_model.train_decoder_on_reconstruction(train_loader)
    
    def apply_style_transfer(self, content_image, style_dict, alpha=0.8):
        """Apply PROPER style transfer using trained AdaIN"""
        with torch.no_grad():
            device = content_image.device
            style_mean = style_dict['mean'].to(device)
            style_std = style_dict['std'].to(device)
            
            # Apply style transfer
            stylized_image = self.style_transfer_model(content_image, style_mean, style_std, alpha)
            
            # Ensure proper range [0, 1]
            stylized_image = torch.clamp(stylized_image, 0, 1)
            
            return stylized_image

# ============================================================================
# Dataset for Decoder Training
# ============================================================================

class DecoderTrainingDataset(Dataset):
    """Dataset for training the AdaIN decoder"""
    
    def __init__(self, dataset_path, csv_file):
        self.dataset_path = dataset_path
        csv_path = os.path.join(dataset_path, csv_file)
        self.df = pd.read_csv(csv_path)
        
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.Grayscale(num_output_channels=3),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image_filename = row['image_path']
        
        # Handle different dataset formats
        if ' ' in image_filename:
            if '(' in image_filename:
                # BUSI format
                class_type = image_filename.split()[0]
                img_path = os.path.join(self.dataset_path, class_type, 'image', image_filename)
            else:
                # BUS-UCLM format
                class_type = image_filename.split()[0]
                image_name = image_filename.split()[1]
                img_path = os.path.join(self.dataset_path, class_type, 'images', image_name)
        else:
            img_path = os.path.join(self.dataset_path, image_filename)
        
        image = Image.open(img_path).convert('RGB')  # Convert to RGB for VGG encoder!
        image_tensor = self.transform(image)
        
        return image_tensor

# ============================================================================
# CCST Dataset for Style Transfer
# ============================================================================

class CCSTDataset(Dataset):
    """
    Dataset for CCST style transfer using PROPER AdaIN.
    """
    
    def __init__(self, source_dataset_path, source_csv, target_style_dict, 
                 output_dir, transform=None):
        self.source_dataset_path = source_dataset_path
        self.target_style_dict = target_style_dict
        self.output_dir = output_dir
        
        # Load source CSV
        csv_path = os.path.join(source_dataset_path, source_csv)
        self.df = pd.read_csv(csv_path)
        
        # Initialize style extractor
        self.style_extractor = CCSTStyleExtractor(device)
        
        # Image transforms
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.Grayscale(num_output_channels=3),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transform
        
        print(f"ðŸŽ¨ PROPER CCST Dataset initialized:")
        print(f"   Source: {source_dataset_path} ({len(self.df)} samples)")
        print(f"   Output: {output_dir}")
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        # Get source image info
        row = self.df.iloc[idx]
        image_filename = row['image_path']
        mask_filename = row['mask_path']
        
        # Handle different dataset formats
        if ' ' in image_filename:
            if '(' in image_filename:
                # BUSI format
                class_type = image_filename.split()[0]
                image_path = os.path.join(self.source_dataset_path, class_type, 'image', image_filename)
                mask_path = os.path.join(self.source_dataset_path, class_type, 'mask', mask_filename)
            else:
                # BUS-UCLM format
                class_type = image_filename.split()[0]
                image_name = image_filename.split()[1]
                mask_name = mask_filename.split()[1]
                image_path = os.path.join(self.source_dataset_path, class_type, 'images', image_name)
                mask_path = os.path.join(self.source_dataset_path, class_type, 'masks', mask_name)
        else:
            # Fallback format
            image_path = os.path.join(self.source_dataset_path, image_filename)
            mask_path = os.path.join(self.source_dataset_path, mask_filename)
        
        # Load original image and mask
        image = Image.open(image_path).convert('RGB')  # Convert to RGB for VGG encoder!
        mask = Image.open(mask_path).convert('L')
        
        # Apply PROPER style transfer
        try:
            if self.transform:
                image_tensor = self.transform(image).unsqueeze(0).to(self.style_extractor.device)
                
                # Apply PROPER AdaIN style transfer
                stylized_tensor = self.style_extractor.apply_style_transfer(
                    image_tensor, self.target_style_dict, alpha=0.8
                )
                
                # Convert back to PIL (grayscale)
                stylized_tensor = stylized_tensor.squeeze(0).cpu().clamp(0, 1)
                stylized_image = to_pil_image(stylized_tensor)
                
                # Convert to grayscale if needed
                if stylized_image.mode != 'L':
                    stylized_image = stylized_image.convert('L')
            else:
                stylized_image = image
                
        except Exception as e:
            print(f"   âš ï¸ Style transfer failed for {image_filename}: {e}")
            stylized_image = image
        
        # Prepare output paths
        styled_filename = f"styled_{image_filename}"
        styled_mask_filename = f"styled_{mask_filename}"
        
        # Create output directories
        if ' ' in image_filename and '(' in image_filename:
            class_type = image_filename.split()[0]
        elif ' ' in image_filename:
            class_type = image_filename.split()[0]
        else:
            class_type = 'unknown'
        
        output_image_dir = os.path.join(self.output_dir, class_type, 'image')
        output_mask_dir = os.path.join(self.output_dir, class_type, 'mask')
        os.makedirs(output_image_dir, exist_ok=True)
        os.makedirs(output_mask_dir, exist_ok=True)
        
        # Save styled image and copy mask
        styled_image_path = os.path.join(output_image_dir, styled_filename)
        styled_mask_path = os.path.join(output_mask_dir, styled_mask_filename)
        
        stylized_image.save(styled_image_path)
        mask.save(styled_mask_path)
        
        return {
            'styled_image_path': styled_image_path,
            'styled_mask_path': styled_mask_path,
            'original_filename': image_filename
        }

# ============================================================================
# Main CCST Pipeline
# ============================================================================

def run_ccst_pipeline(source_dataset, source_csv, target_dataset, target_csv, 
                     output_dir, K=1, J_samples=None):
    """
    Run the complete CCST pipeline with PROPER AdaIN implementation
    """
    print("ðŸš€ Starting PROPER CCST Pipeline - High-Quality Medical Style Transfer")
    print("ðŸŽ¨ Direction: Extract BUSI style â†’ Apply to BUS-UCLM content")
    print("=" * 80)
    
    # Step 1: Extract target domain style (BUSI)
    print(f"\nðŸ“Š Step 1: Extracting BUSI style from {target_dataset}...")
    style_extractor = CCSTStyleExtractor(device)
    
    # Train decoder first for high-quality reconstruction
    print("ðŸŽ“ Training decoder for high-quality reconstruction...")
    style_extractor.train_decoder_with_data(target_dataset, target_csv, batch_size=4)
    
    target_style = style_extractor.extract_overall_domain_style(
        target_dataset, target_csv, J_samples
    )
    
    # Step 2: Apply style transfer to source domain (BUS-UCLM)
    print(f"\nðŸŽ¨ Step 2: Applying BUSI style to BUS-UCLM content from {source_dataset}...")
    
    # Create CCST dataset
    ccst_dataset = CCSTDataset(
        source_dataset, source_csv, target_style, output_dir
    )
    
    # Process all images with progress bar
    styled_data = []
    for i in tqdm(range(len(ccst_dataset)), desc="Applying PROPER BUSI style to BUS-UCLM"):
        result = ccst_dataset[i]
        styled_data.append({
            'styled_image_path': result['styled_image_path'],
            'styled_mask_path': result['styled_mask_path']
        })
    
    # Step 3: Create styled dataset CSV
    print(f"\nðŸ“‹ Step 3: Creating BUSI-styled BUS-UCLM dataset CSV...")
    styled_df = pd.DataFrame(styled_data)
    styled_csv_path = os.path.join(output_dir, 'styled_dataset.csv')
    styled_df.to_csv(styled_csv_path, index=False)
    
    print(f"   âœ… Created styled dataset CSV: {styled_csv_path}")
    print(f"   ðŸ“Š Total BUSI-styled BUS-UCLM samples: {len(styled_df)}")
    
    # Step 4: Summary
    print(f"\nðŸ“ˆ Step 4: PROPER CCST Pipeline Summary")
    print("=" * 80)
    print(f"âœ… Style source (BUSI): {target_dataset}")
    print(f"âœ… Content source (BUS-UCLM): {source_dataset}")
    print(f"âœ… Decoder training: Completed with medical images")
    print(f"âœ… Style extraction samples: {'All' if not J_samples else J_samples}")
    print(f"âœ… HIGH-QUALITY styled images saved to: {output_dir}")
    print(f"âœ… Styled dataset CSV: {styled_csv_path}")
    print(f"âœ… Total BUSI-styled BUS-UCLM samples: {len(styled_df)}")
    
    print(f"\nðŸŽ¯ Training Data Summary:")
    print(f"   Original BUSI training: ~400 images")
    print(f"   HIGH-QUALITY BUSI-styled BUS-UCLM: {len(styled_df)} images")
    print(f"   Total BUSI-style training data: ~{400 + len(styled_df)} images")
    
    print(f"\nðŸŽ¯ Next Steps:")
    print("1. Train with: Original BUSI + HIGH-QUALITY BUSI-styled BUS-UCLM")
    print("2. Test on: Original BUSI test set")
    print("3. Expected improvement based on CCST paper:")
    print("   - Dice Score: +9.16% improvement")
    print("   - IoU: +9.46% improvement")
    print("   - Hausdorff Distance: -17.28% improvement")

def main():
    parser = argparse.ArgumentParser(description='PROPER CCST Style Transfer for Medical Images')
    parser.add_argument('--source_dataset', required=True, help='Source dataset path (BUS-UCLM)')
    parser.add_argument('--source_csv', required=True, help='Source dataset CSV file')
    parser.add_argument('--target_dataset', required=True, help='Target dataset path (BUSI)')
    parser.add_argument('--target_csv', required=True, help='Target dataset CSV file')
    parser.add_argument('--output_dir', required=True, help='Output directory for styled images')
    parser.add_argument('--K', type=int, default=1, help='Number of style transfers per image')
    parser.add_argument('--J_samples', type=int, default=None, help='Number of samples for style extraction')
    
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Run CCST pipeline
    run_ccst_pipeline(
        args.source_dataset, args.source_csv,
        args.target_dataset, args.target_csv,
        args.output_dir, args.K, args.J_samples
    )

if __name__ == "__main__":
    main() 